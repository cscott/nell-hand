Some accuracy numbers.
 "xN mixtures" means "exactly N mixtures per state".
 " N mixtures" means "average N mixtures per state, power 0.2".
 Accuracies in square brackets are test data accuracies; all others are
 for training data.

Initial features -- no pen up feature:
  Unknown states, unknown mixtures: 83.58% (hmm8)
 x3 mixtures, 12 states:            87.17% (hmmE)

Pen Up feature, no delta or acceleration features:
 x3 mixtures, 12 states:            86.48% (hmmE)
 x5 mixtures, 12 states:            87.84% (hmmH)

Pen Up feature, plus delta and acceleration features:
 x5 mixtures, 12 states:            90.07% (hmmN)
  9 mixtures,  6 states:            88.69% (hmmN)
  1 mixtures,  8 states:            81.29% (hmm9) [80.06%]
  2 mixtures,  8 states:            84.18% (hmmR) [83.48%]
  4 mixtures,  8 states:            86.61% (hmmU) [85.75%]
  6 mixtures,  8 states:            87.98% (hmmX) [87.10%]
  8 mixtures,  8 states:            89.61% (hmm10)[88.77%]
  9 mixtures,  8 states:            89.90% (hmm13)[89.19%] (was 90.56% w hmmN)
 10 mixtures,  8 states:            90.36% (hmm16)[89.56%]
 12 mixtures,  8 states:            90.94% (hmm19)[90.08%]
 14 mixtures,  8 states:            91.42% (hmm1C)[90.23%]
 16 mixtures,  8 states:            91.91% (hmmO) [90.54%]
 18 mixtures,  8 states:            92.31% (hmmR) [90.96%]
 20 mixtures,  8 states:            92.55% (hmmU) [91.05%]
 22 mixtures,  8 states:            92.78% (hmmX) [91.38%]
 24 mixtures,  8 states:            92.99% (hmm10)[91.43%]
 26 mixtures,  8 states:            93.20% (hmm13)[91.59%]
 28 mixtures,  8 states:            93.42% (hmm16)[91.41%]
 30 mixtures,  8 states:            93.60% (hmm19)[91.70%]
 32 mixtures,  8 states:            93.96% (hmm1E)[91.78%]
  9 mixtures, 10 states:            91.84% (hmmN)
  1 mixtures, 12 states:            85.18% (hmm9) [84.46%]
  9 mixtures, 12 states:            92.15% (hmmX) [90.87%] (was 92.57% (hmmN))
 16 mixtures, 12 states:            94.05% (hmmX) [92.54%]
 24 mixtures, 12 states:            95.29% (hmmX) [93.27%]
  9 mixtures, 14 states:            93.41% (hmmN)
  1 mixture,  16 states:            85.77% (hmm9) [84.44%]
  9 mixtures, 16 states:            94.17% (hmmN)
 12 mixtures, 16 states:            94.50% (hmmN)
 12 mixtures, 16 states:            94.59% (hmmO)
 16 mixtures, 16 states:            95.40% (hmmO) [93.45%]
 18 mixtures, 16 states:            95.52% (hmmR) [93.67%]
 20 mixtures, 16 states:            95.77% (hmmU) [93.76%]
 22 mixtures, 16 states:            95.91% (hmmX) [93.90%]
 24 mixtures, 16 states:            96.06% (hmm10)[94.00%]
 26 mixtures, 16 states:            96.18% (hmm13)[94.07%]
 28 mixtures, 16 states:            96.28% (hmm16)[94.14%]
 30 mixtures, 16 states:            96.36% (hmm19)[94.29%]
 32 mixtures, 16 states:            96.63% (hmm1E)[94.16%] *overtraining*

Same features, experimenting with alternate topologies:
 5x2  topo == 10 states, like two 6 state models in parallel
  1 mixture,  10 states:            76.08%
  9 mixtures, 10 states:            88.69%
 5x2x topo == 10 states, like two 6 state models in parallel *WITH CROSSOVER*
  1 mixture,  10 states:            76.60%
  9 mixtures, 10 states:            90.99%
 6x2  topo == 12 states, like two 7 state models in parallel
  1 mixture,  12 states:            78.81%
  9 mixtures, 12 states:            90.24%
 6x2x topo == 12 states, like two 7 state models in parallel *WITH 2 CROSSOVER*
  1 mixture,  12 states:            77.48%
  9 mixtures, 12 states:            92.16%
 7x2  topo == 14 states, like two 8 state models in parallel
  1 mixture,  14 states:            81.30%
  9 mixtures, 14 states:            90.56% (compared to 90.56% for 9 mix 8 st)
   -- identical results suggest that parallel models are identical, ie
      training technique used for this series isn't actually separating
      input into two classes.
 7x2x topo == 14 states, like two 8 state models in parallel *WITH CROSSOVER*
  1 mixture,  14 states:            81.65% (compared to 81.29% for 1 mix 8 st)
  9 mixtures, 14 states:            93.17% (compared to 90.56% for 9 mix 8 st)
                                           (compared to 93.41% for 9 mix 14 st)

Two allographs:
  1 mixture,   8 states:            84.84% [84.33%] (1 allo = 81.29% [80.06%])
* 9 mixtures,  8 states:            92.62% [91.78%] (1 allo = 89.90% [89.19%]) (hmmT, hmmU-X overt'ed)
 24 mixtures,  8 states:            95.60% [93.16%] (1 allo = 92.99% [91.43%])
  1 mixture,  12 states:            87.90% [87.61%] (1 allo = 85.18% [84.46%])
  9 mixtures, 12 states:            94.67% [92.67%] (1 allo = 92.15% [90.87%])
 16 mixtures, 12 states:            96.02% [93.32%] (1 allo = 94.05% [92.54%])
 24 mixtures, 12 states:            96.98% [93.89%] (1 allo = 95.29% [93.27%])
  1 mixture,  16 states:            88.40% [88.05%] (1 allo = 85.77% [84.44%])
* 9 mixtures, 16 states:            95.54% [93.45%] (1 allo = 94.17%         )
 16 mixtures, 16 states:            96.74% [94.18%] (1 allo = 95.40% [93.45%])
 24 mixtures, 16 states:            97.49% [94.36%] (1 allo = 96.06% [94.00%])

Three allographs:
  1 mixture,   8 states:            86.48% [85.55%] (1 allo = 81.29% [80.06%])
 24 mixtures,  8 states:            96.66% [93.40%] (1 allo = 92.99% [91.43%])
  1 mixture,  12 states:            89.61% [89.34%] (1 allo = 85.18% [84.46%])
 24 mixtures, 12 states:            97.59% [94.18%] (1 allo = 95.29% [93.27%])

Four allographs:
  1 mixture,   8 states:            88.07% [86.50%] (1 allo = 81.29% [80.06%])
  9 mixtures,  8 states:            95.24% [92.85%] (1 allo = 89.90% [89.19%])
 16 mixtures,  8 states:            96.44% [93.43%] (1 allo = 91.91% [90.54%])
 24 mixtures,  8 states:            97.30% [93.63%] (1 allo = 92.99% [91.43%])
  1 mixture,  12 states:            90.03% [89.23%] (1 allo = 85.18% [84.46%])
 24 mixtures, 12 states:            98.19% [93.92%] (1 allo = 95.29% [93.27%])
  1 mixture,  16 states:            91.67% [90.87%] (1 allo = 85.77% [84.44%])
  9 mixtures, 16 states:            97.28% [94.32%] (1 allo = 94.17%         )
 16 mixtures, 16 states:            98.03% [94.01%] (1 allo = 95.40% [93.45%])
 24 mixtures, 16 states:            98.49% [94.00%] (1 allo = 96.06% [94.00%])

Six allographs:
  1 mixture,   8 states:            90.58% [89.19%] (1 allo = 81.29% [80.06%])
 24 mixtures,  8 states:            98.14% [93.67%] (1 allo = 92.99% [91.43%])
  1 mixture,  12 states:            92.66% [91.19%] (1 allo = 85.18% [84.46%])
 24 mixtures, 12 states:            98.60% [93.72%] (1 allo = 95.29% [93.27%])

Eight allographs:
  1 mixture,   8 states:            90.91% [89.63%] (1 allo = 81.29% [80.06%])
 24 mixtures,  8 states:            98.41% [93.65%] (1 allo = 92.99% [91.43%])
  1 mixture,  12 states:            93.72% [92.25%] (1 allo = 85.18% [84.46%])
 24 mixtures, 12 states:            98.70% [93.16%] (1 allo = 95.29% [93.27%])

============= DISCRETE HMMs ==============


DISCRETE: treevq-euclidean distance 64/64/16
  8 states, 4 allographs:           93.01% [90.76%]
DISCRETE: treevq-euclidean distance 128/64/16
  8 states, 4 allographs:           94.14% [91.23%]
DISCRETE: treevq-euclidean distance 128/64/64
 12 states, 6 allographs:           96.09% [92.25%]
DISCRETE: treevq-euclidean distance 256/64/16
* 8 states, 2 allographs:           92.98% [90.74%] (93.23 [90.59] w/o -w 1)
  8 states, 4 allographs:           94.70% [91.43%]
  8 states, 6 allographs:           95.55% [92.03%]
  8 states, 8 allographs:           96.00% [92.29%]
 12 states, 4 allographs:           95.37% [92.12%]
 16 states, 4 allographs:           95.93% [92.39%]
DISCRETE: treevq-euclidean distance 256/64/64
  8 states, 4 allographs:           95.24% [91.94%] (slightly overtrained)
DISCRETE: treevq-euclidean distance 512/64/16
  8 states, 4 allographs:           95.16% [91.74%]
DISCRETE: treevq-diagonal mahalanobis distance 256/64/16
  8 states, 2 allographs:           90.48% [86.95%]
  8 states, 4 allographs:           93.70% [88.19%]
DISCRETE: linvq-euclidean distance 128/64/64
 12 states, 6 allographs:           95.96% [93.60%]
DISCRETE: linvq-euclidean distance 256/64/16
  8 states, 2 allographs:           93.25% [91.10%]
  8 states, 4 allographs:           95.06% [92.47%]
  8 states, 6 allographs:           95.97% [92.94%]
  8 states, 8 allographs:           96.23% [93.29%]
 12 states, 4 allographs:           95.61% [92.94%]
 16 states, 4 allographs:           96.08% [93.34%]

============= TIED MIXTURE HMMs: tie the 1 mixture HMM ================
256/64/64 codebook, diagonal gaussian (2.0 floor)
  8 states, 4 allographs:           93.09% [90.68%]
 12 states, 4 allographs:           96.58% [93.47%] (hmmG)
 16 states, 4 allographs:           97.45% [94.65%] (hmmG) (no overtraining)
256/128/64 codebook, diagonal gaussian (1.5 floor).
  8 states, 4 allographs:           92.79% [90.86%] (hmmD) (hmmE-G overtrained)
512/128/128 codebook, diagonal gaussian (2.0 floor)
  8 states, 4 allographs:           94.01% [91.08%] (hmmD) (hmmE-G overtrained)
 16 states, 4 allographs:           97.40% [94.62%] (hmmE) (hmmF-G overtrained)



==== TIED MIXTURES take 2: train to multiple mixtures before tying ====
8 states, 2 allographs, trained to  5 mixtures then tied w/ diag gaussians. 2.0 floor.
  1 mix => 84.84% [84.33%];  5 mix => 91.46% [90.47%];
*    tied 256/128/128 => 92.33% [91.03%] (hmm11) (hmm12 overtrained)
8 states, 4 allographs, trained to  9 mixtures then tied w/ diag gaussians. 2.0 floor.
  1 mix => 88.07% [86.50%];  9 mix => 95.24% [92.85%];
     tied 256/ 64/ 64 => 94.31% [91.75%] (hmm11) (hmm12 overtrained)
     tied 256/128/128 => 93.97% [92.07%] (hmm10) (hmm11-12 overtrained)
     tied 512/ 64/ 64 => 95.18% [92.17%] (hmm11) (hmm12 overtrained)
     tied 512/128/128 => 95.27% [92.34%] (hmm11) (hmm12 overtrained)
8 states, 4 allographs, trained to 16 mixtures then tied w/ diag gaussians.
  1 mix => 88.07% [86.50%]; 16 mix => 96.44% [93.43%];
     tied 256/ 64/ 64 => 93.27% [91.22%] (hmm11) (hmm12 overtrained)
     tied 256/128/128 => 93.81% [91.68%] (hmm10) (hmm11-12 overtrained)
     tied 512/ 64/ 64 => 94.61% [91.87%] (hmm11) (hmm12 overtrained)
     tied 512/128/128 => 95.10% [92.16%] (hmm11) (hmm12 overtrained)
